<html>
    <head>
        <title>Dorian Chan</title>
        <style>
            body {
                font-family: Arial, Helvetica, sans-serif;
                color: rgb(33, 37, 41);
            }
            h1, b, h3 {
                color: black;
            }
            .content {
                margin: 0 auto;
                max-width: 800px;
            }
            #profile_pic {
                width: 200px;
                clip-path: circle();
            }
            #main_left {
                flex-grow: 1;
            }
            #main {
                display: flex;
                flex-direction: row;
                align-items: center;
            }
            #main_right {
                padding-left: 20px;
                flex-grow: 2;
                color: black;
            }
            #main_right h1 {
                margin-top: 0px;
            }
            .paperc {
                margin-bottom: 30px;
            }
            .paper {
                display: flex;
                flex-direction: row;
                align-items: center;
            }
            .paper_left {
                padding-right: 20px;
            }
            .paper_left > img {
                width: 120px;
                max-height: 100px;
                object-fit: cover;
                border: 1px solid gray;
            }
            .paperc > p {
                color: #666666;
            }

            a {
                color: rgb(10, 88, 202);
            }

        </style>
    </head>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-66PZ9T1D90"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-66PZ9T1D90');
    </script>

    <body>
        <div class="content">
            <div id="main">
                <div id="main_left">
                    <img src="images/me.jpg" id="profile_pic"></img>
                </div>
                <div id="main_right">
                    <h1>Dorian Chan</h1>
                    <div>| Computational Photography Researcher |</div>
                    <div>| Fishing Devotee | | Musician Wannabe |</div>
                    <div>| <a href="https://scholar.google.com/citations?user=rgf7oH4AAAAJ" target="_blank">Google Scholar</a> | | <a href="https://www.linkedin.com/in/dorian-chan-765792117/" target="_blank">Linkedin</a> |</div>
                </div>
            </div>
            <br/>
            
            <p>I'm an AI Resident at Apple working on computational photography. I recently received my PhD from the Computer Science Department at Carnegie Mellon University, where I was advised by the wonderful <a href="http://www.cs.cmu.edu/~motoole2/">Matthew O'Toole</a>. I've also previously worked as a research intern at Meta Reality Labs Research and Snap Research.</p>

            <p>My research focuses on rethinking the conventional camera in order to improve real-world robustness, reduce computation time and potentially even capture phenomena traditionally invisible to the human eye. In short, I think deeply about the underlying physical limits of modern camera hardware, and how computational tools can be used to fill the gaps. This joint perspective inspired my work on <a href="https://imaging.cs.cmu.edu/spincam/">extremely cheap high-speed cameras</a>, <a href="https://dl.acm.org/doi/abs/10.1145/3610548.3618152">far more robust and energy-efficient LiDAR systems</a>, <a href="https://imaging.cs.cmu.edu/vibration/">cameras that can see vibrations</a>, and more. My work has been recognized with the <a href="https://cvpr2022.thecvf.com/cvpr-2022-paper-awards">Best Paper Honorable Mention Award at CVPR 2022</a>, and  <a href="https://s2020.siggraph.org/conference/program-events/organization-events/student-research-competition/">1st place at the SIGGRAPH 2020 Student Research Competition</a>. I have also served as a reviewer for CVPR, ICCV, ECCV, TOG, ICCP and TCI.</p>

            <!-- <p>I'm also currently an ombudsperson for the Computer Science Department. Please feel free to reach out if there's anything you'd like to chat about!</p> -->

            <p>I quite enjoy <a href="https://fishoak.com/">fishing</a>, and on occasion I try to make <a href="https://www.youtube.com/channel/UCQ2EUQXiAYtUOB3JuleWhow">music</a>. I'm also a big Denver Nuggets fan.</p>
            <p>drop me a line: <b>dorianychan@gmail.com</b></p>
            
            <!-- <p><b style="color:red">I am looking for academic or industry jobs starting Summer or Fall 2025. Let me know if you have any leads!</b></p> -->
            <h3>Some of my past work:</h3>
            <div class="paperc">
                <div class="paper">
                    <div class="paper_left"> <video width="120" autoplay loop>
                        <source src="papers/holospeed.mp4" type="video/mp4">
                    </video> </div>
                    
                    <div class="paper_right">
                        <div><b><a href="holospeed">Holospeed: High-Speed Holographic Displays for Dynamic Content</a></b></div>
                        <div><b>Dorian Chan</b>, Oliver Cossairt, Nathan Matsuda, Grace Kuo</div>
                        <div><i>ICCP 2025</i></div>
                    </div>
                </div>
                <p>We explore the perceptual impacts of time-multiplexed holographic displays. While traditional architectures suffer from significant blurring and strobing effects, we show that a paradigm based on treating a traditionally time-multiplexed holographic system as a <b>noisy high-speed display</b> produces artifact-free output.</p>
            </div>

            <div class="paperc">
                <div class="paper">
                    <div class="paper_left"><img src="papers/projector_both.svg" /></div>
                    
                    <div class="paper_right">
                        <div><b><a href="holodepth">Holodepth: Programmable Depth-Varying Projection via Computer-Generated Holography</a></b></div>
                        <div><b>Dorian Chan</b>, Matthew O'Toole, Sizhuo Ma, Jian Wang</div>
                        <div><i>ECCV 2024</i></div>
                    </div>
                </div>
                <p>We explore how to build next-generation projectors that are capable of programmably projecting <b>unique content at multiple planes</b> via precisely-controlled laser light. This never-before-seen capability enables new interfaces where users see different content at different distances, and novel methods for depth sensing. Building such a system requires careful modeling, accomplished using AI-driven tools to capture the complexities of holographic light transport.</p>
            </div>

            <div class="paperc">
                <div class="paper">
                    <div class="paper_left"><img src="papers/concentrate.png" /></div>
                    
                    <div class="paper_right">
                        <div><b><a href="https://dl.acm.org/doi/abs/10.1145/3610548.3618152">Light-Efficient Holographic Illumination for Continuous-Wave Time-of-Flight Imaging</a></b></div>
                        <div><b>Dorian Chan</b>, Matthew O'Toole</div>
                        <div><i>SIGGRAPH ASIA 2023</i></div>
                    </div>
                </div>
                <p>We build new light sources for LiDAR systems, that programmably <b>concentrate</b> light using the unique properties of laser light. By intelligently redistributing light according to priors on scene content, e.g., dark versus bright objects, we can build new depth sensors that have far increased depth and dynamic range compared to traditional devices. In our experiments, our system can display patterns at 2700x the brightness of a traditional approach. </p>
            </div>
            
            <div class="paperc">
                <div class="paper">
                    <div class="paper_left"><img src="https://imaging.cs.cmu.edu/spincam/assets/vids/teaser/rendered.gif" /></div>
                    
                    <div class="paper_right">
                        <div><b><a href="https://imaging.cs.cmu.edu/spincam/">SpinCam: High-Speed Imaging via a Rotating Point-Spread Function</a></b></div>
                        <div><b>Dorian Chan</b>, Mark Sheinin, Matthew O'Toole</div>
                        <div><i>ICCV 2023</i></div>
                    </div>
                </div>
                <p>We demonstrate a new methodology towards motion deblurring by designing a <b>time-varying point-spread function</b>. Along with an appropriate compressed sensing-based optimization algorithm, we can recover high-speed 192kHz videos (2x improvement over past work), suitable for tasks like motion capture, particle-image velocimetry and ballistics. </p>
            </div>

            <div class="paperc">
                <div class="paper">
                    <div class="paper_left"><img src="papers/pingpong.gif" /></div>
                    
                    <div class="paper_right">
                        <div><b><a href="https://imaging.cs.cmu.edu/impactlocalization/">Analyzing Physical Impacts using Transient Surface Wave Imaging</a></b></div>
                        <div>Tianyuan Zhang, Mark Sheinin, <b>Dorian Chan</b>, Mark Rau, Matthew O'Toole, and Srinivasa Narasimhan </div>
                        <div><i>CVPR 2023</i></div>
                    </div>
                </div>
                <p>We show how the tiny <b>vibrations</b> captured by our previously proposed "seeing-sound" camera can be used to potentially <b>localize human-object interactions</b>. For instance, we can identify where a ping pong ball bounces on a table,  or where a user steps in a room --- all without needing to directly see the aforementioned event.</p>
            </div>
            
            <div class="paperc">
                <div class="paper">
                    <div class="paper_left"><img src="papers/dualshutter.png" /></div>
                    
                    <div class="paper_right">
                        <div><b><a href="https://imaging.cs.cmu.edu/vibration/">Dual-Shutter Optical Vibration Sensing</a></b></div>
                        <div>Mark Sheinin, <b>Dorian Chan</b>, Matthew O'Toole, Srinivasa Narasimhan</div>
                        <div><i>CVPR 2022</i> - <span style="color:red">Oral, Best Paper Honorable Mention (~Top 2 paper)!!</span></div>
                    </div>
                </div>
                <p>We construct a simple camera system for <b>"seeing sound"</b>, using just a laser, a rolling-shutter and a global-shutter camera (a far cheaper configuration than standard high-speed cameras). We can use this system to separate sound sources in a room, reconstruct human voices that vibrate a bag of chips and visualize the vibrations of a tuning fork.</p>
            </div>

            <div class="paperc">
                <div class="paper">
                    <div class="paper_left"><img src="papers/cow.gif" /></div>
                    
                    <div class="paper_right">
                        <div><b><a href="https://imaging.cs.cmu.edu/holocurtains/">Holocurtains: Programming Light Curtains via Binary Holography </a></b></div>
                        <div><b>Dorian Chan</b>, Srinivasa Narasimhan, Matthew O'Toole</div>
                        <div><i>CVPR 2022</i></div>
                    </div>
                </div>
                 <p>We demonstrate an approach for <b>depth-selective imaging</b>, where a camera only sees objects at a predefined distance away via holographically-manipulated laser light. Such a system enables new applications in human-robot safety, privacy-preserving cameras and optical touchscreens.</p>
            </div>

            <div class="paperc">
                <div class="paper">
                    <div class="paper_left"><img src="papers/icon_nlos.png" /></div>
                    
                    <div class="paper_right">
                        <div><b><a href="https://arxiv.org/abs/2008.02787">Efficient Non-Line-of-Sight Imaging from Transient Sinograms</a></b></div>
                        <div>Mariko Isogawa, <b>Dorian Yao Chan</b>, Ye Yuan, Kris Kitani, Matthew O'Toole</div>
                        <div><i>ECCV 2020</i></div>
                    </div>
                </div>
                <p>We explore the limits of <b>"seeing around corners"</b>, by showing that a circular LiDAR scan of points can be effectively used for non-line-of-sight imaging via the projection-slice theorem or a careful construction of ADMM. Our work demonstrates explicit connections between NLOS and computed tomography, a relationship that has long been speculated upon but never before been proven.</p>
            </div>

            <div class="paperc">
                <div class="paper">
                    <div class="paper_left"><img src="papers/icon_hdr.png" /></div>
                    
                    <div class="paper_right">
                        <div><b><a href="https://dl.acm.org/doi/abs/10.1145/3388770.3407415">Bound-Constrained Optimized Dynamic Range Compression</a></b></div>
                        <div><b>Dorian Yao Chan</b>, James F. O'Brien </div>
                        <div><i>SIGGRAPH'20 Posters</i> - <span style="color:red">ACM Student Research Competition winner!!</span></div>
                    </div>
                </div>
                <p>We create a new framework for <b>high-dynamic range tonemapping</b>, that uses modern gradient-descent tools to optimally match the image perceived on a standard monitor with a desired HDR scene in-the-wild. Our approach improved perceptual quality by 4% over the previous state-of-the-art in a user study.</p>
            </div>

            <div class="paperc">
                <div class="paper">
                    <div class="paper_left"><img src="papers/icon_bci.png" /></div>
                    
                    <div class="paper_right">
                        <div><b><a href="https://ieeexplore.ieee.org/abstract/document/8717158">Brain-Computer Interface in Virtual Reality</a></b></div>
                        <div>Reza Abbasi-Asl, Mohammad Keshavarzi, <b>Dorian Yao Chan</b></div>
                        <div><i>NER'19</i></div>
                    </div>
                </div>
                <p>We explore how electroencephalography devices can be used to control virtual reality devices with a thought.</p>
            </div>

            <div class="paperc">
                <div class="paper">
                    <div class="paper_left"><img src="papers/icon_svbrdf.png" /></div>
                    
                    <div class="paper_right">
                        <div><b><a href="http://graphics.berkeley.edu/papers/Albert-ASV-2018-06/">Approximate svBRDF Estimation From Mobile Phone Video</a></b></div>
                        <div>Rachel A. Albert, <b>Dorian Yao Chan</b>, Dan B Goldman, James F. O'Brien </div>
                        <div><i>EGSR 2018</i></div>
                    </div>
                </div>
                <p>We develop a new methodology for digitizing real-world materials for later rendering using just mobile phone videos.</p>
            </div>

            <!-- <h3>Other:</h3>
            <p>
                <div><b><a href="kirchhoff_paper.pdf">On F-K migration and non-line-of-sight imaging</a></b></div>
                <div>Implementing Kirchhoff migration in NLOS imaging, and some theoretical analyses of F-K migration in relation to traditional NLOS approaches. Lots of math, and some of the 
                equations may be slightly incorrect.</div>
            </p> -->
        </div>
    </body>
</html>